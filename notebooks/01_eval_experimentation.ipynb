{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iofold Eval Experimentation Notebook\n",
    "\n",
    "This notebook provides a sandbox for experimenting with different eval strategies on trace data from the iofold database.\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Make sure the backend is running (`pnpm run dev` in project root)\n",
    "2. Install dependencies: `pip install -r requirements.txt`\n",
    "3. Run cells in order\n",
    "\n",
    "## What you can do here:\n",
    "- Explore trace data structure\n",
    "- Write custom eval functions in Python\n",
    "- Test eval functions against labeled traces\n",
    "- Compare eval strategies with statistical metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any\n",
    "\n",
    "# Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, f1_score, accuracy_score\n",
    "\n",
    "# LLM clients (optional, for LLM-as-judge evals)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import anthropic\n",
    "    ANTHROPIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ANTHROPIC_AVAILABLE = False\n",
    "\n",
    "# Local utilities\n",
    "import db_utils\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"OpenAI available: {OPENAI_AVAILABLE}\")\n",
    "print(f\"Anthropic available: {ANTHROPIC_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the database\n",
    "tables = db_utils.list_tables()\n",
    "print(\"Database Tables:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trace statistics\n",
    "stats = db_utils.get_trace_statistics()\n",
    "print(\"\\nTrace Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View traces table schema\n",
    "print(\"\\nTraces Table Schema:\")\n",
    "display(db_utils.get_table_schema('traces'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch recent traces\n",
    "traces_df = db_utils.query(\"\"\"\n",
    "    SELECT \n",
    "        t.id,\n",
    "        t.trace_id,\n",
    "        t.source,\n",
    "        t.timestamp,\n",
    "        t.step_count,\n",
    "        t.has_errors,\n",
    "        t.input_preview,\n",
    "        t.output_preview,\n",
    "        f.rating as feedback_rating,\n",
    "        f.rating_detail\n",
    "    FROM traces t\n",
    "    LEFT JOIN feedback f ON f.trace_id = t.id\n",
    "    ORDER BY t.imported_at DESC\n",
    "    LIMIT 20\n",
    "\"\"\", as_df=True)\n",
    "\n",
    "print(f\"Found {len(traces_df)} traces\")\n",
    "display(traces_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single trace in detail\n",
    "if len(traces_df) > 0:\n",
    "    sample_trace_id = traces_df.iloc[0]['id']\n",
    "    trace = db_utils.get_trace_by_id(sample_trace_id)\n",
    "    \n",
    "    if trace:\n",
    "        print(\"Sample Trace Details:\")\n",
    "        db_utils.print_trace_summary(trace)\n",
    "        \n",
    "        print(\"\\n--- Steps Structure ---\")\n",
    "        for i, step in enumerate(trace.steps[:3]):  # First 3 steps\n",
    "            print(f\"\\nStep {i}:\")\n",
    "            print(f\"  Messages: {len(step.get('messages_added', []))}\")\n",
    "            print(f\"  Tool Calls: {len(step.get('tool_calls', []))}\")\n",
    "            if step.get('error'):\n",
    "                print(f\"  Error: {step['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Labeled Traces\n",
    "\n",
    "Labeled traces have human feedback (positive/negative rating) that we can use to train and test eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labeled traces\n",
    "labeled_traces = db_utils.get_labeled_traces(limit=100)\n",
    "print(f\"Found {len(labeled_traces)} labeled traces\")\n",
    "\n",
    "if labeled_traces:\n",
    "    labeled_df = pd.DataFrame(labeled_traces)\n",
    "    print(\"\\nRating distribution:\")\n",
    "    print(labeled_df['rating'].value_counts())\n",
    "    \n",
    "    print(\"\\nHuman score distribution:\")\n",
    "    print(labeled_df['human_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing Custom Eval Functions\n",
    "\n",
    "Eval functions follow this signature:\n",
    "```python\n",
    "def eval_function(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    # Returns: (score between 0.0 and 1.0, feedback string)\n",
    "    return (score, feedback)\n",
    "```\n",
    "\n",
    "For notebook experimentation, we'll use a simplified version without the ctx parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock context for eval functions (simplified version)\n",
    "class MockEvalContext:\n",
    "    \"\"\"Mock context for testing eval functions in notebooks.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_client=None, anthropic_client=None):\n",
    "        self.openai = openai_client\n",
    "        self.anthropic = anthropic_client\n",
    "        self._cost = 0.0\n",
    "        self._cache = {}\n",
    "    \n",
    "    def call_llm(self, prompt: str, model: str = \"gpt-4o-mini\", \n",
    "                 temperature: float = 0.0, max_tokens: int = 500,\n",
    "                 cache_key: str = None) -> str:\n",
    "        \"\"\"Call LLM for semantic evaluation (optional).\"\"\"\n",
    "        # Check cache first\n",
    "        if cache_key and cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        if self.openai and 'gpt' in model.lower():\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "        elif self.anthropic and 'claude' in model.lower():\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            result = response.content[0].text\n",
    "        else:\n",
    "            raise ValueError(f\"No client available for model: {model}\")\n",
    "        \n",
    "        # Cache result\n",
    "        if cache_key:\n",
    "            self._cache[cache_key] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cost_so_far(self) -> float:\n",
    "        return self._cost\n",
    "    \n",
    "    def get_remaining_budget(self) -> float:\n",
    "        return 0.05 - self._cost\n",
    "\n",
    "# Initialize context (optional - add your API keys if needed)\n",
    "ctx = MockEvalContext()\n",
    "print(\"Mock context created. Add API clients if you need LLM-based evals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Eval Function 1: Simple heuristic-based eval\n",
    "def eval_response_length(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Simple eval: Score based on response length.\n",
    "    Longer responses get higher scores (up to a point).\n",
    "    \"\"\"\n",
    "    response = db_utils.extract_assistant_response(trace)\n",
    "    \n",
    "    if not response:\n",
    "        return (0.0, \"No assistant response found\")\n",
    "    \n",
    "    length = len(response)\n",
    "    \n",
    "    # Score based on length brackets\n",
    "    if length < 50:\n",
    "        score = 0.2\n",
    "        feedback = \"Response too short\"\n",
    "    elif length < 200:\n",
    "        score = 0.5\n",
    "        feedback = \"Response is brief\"\n",
    "    elif length < 500:\n",
    "        score = 0.8\n",
    "        feedback = \"Response is well-developed\"\n",
    "    else:\n",
    "        score = 1.0\n",
    "        feedback = \"Response is comprehensive\"\n",
    "    \n",
    "    return (score, f\"{feedback} ({length} chars)\")\n",
    "\n",
    "print(\"Eval function 'eval_response_length' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Eval Function 2: Error detection\n",
    "def eval_no_errors(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Eval: Penalize traces with errors.\n",
    "    \"\"\"\n",
    "    steps = trace.get('steps', [])\n",
    "    if isinstance(steps, str):\n",
    "        steps = json.loads(steps)\n",
    "    \n",
    "    error_count = 0\n",
    "    error_messages = []\n",
    "    \n",
    "    for step in steps:\n",
    "        if step.get('error'):\n",
    "            error_count += 1\n",
    "            error_messages.append(step['error'][:50])\n",
    "        \n",
    "        for tc in step.get('tool_calls', []):\n",
    "            if tc.get('error'):\n",
    "                error_count += 1\n",
    "                error_messages.append(tc['error'][:50])\n",
    "    \n",
    "    if error_count == 0:\n",
    "        return (1.0, \"No errors detected\")\n",
    "    elif error_count <= 2:\n",
    "        return (0.5, f\"Minor errors: {'; '.join(error_messages[:2])}\")\n",
    "    else:\n",
    "        return (0.0, f\"Multiple errors ({error_count}): {'; '.join(error_messages[:3])}\")\n",
    "\n",
    "print(\"Eval function 'eval_no_errors' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Eval Function 3: Tool usage efficiency\n",
    "def eval_tool_efficiency(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Eval: Score based on reasonable tool usage.\n",
    "    Penalize too few or too many tool calls.\n",
    "    \"\"\"\n",
    "    tool_calls = db_utils.extract_tool_calls(trace)\n",
    "    num_tools = len(tool_calls)\n",
    "    \n",
    "    if num_tools == 0:\n",
    "        # No tools - might be okay for simple tasks\n",
    "        return (0.6, \"No tool calls - may be appropriate for simple queries\")\n",
    "    elif num_tools <= 3:\n",
    "        return (1.0, f\"Efficient tool usage ({num_tools} calls)\")\n",
    "    elif num_tools <= 7:\n",
    "        return (0.7, f\"Moderate tool usage ({num_tools} calls)\")\n",
    "    else:\n",
    "        return (0.4, f\"Excessive tool usage ({num_tools} calls) - may indicate inefficiency\")\n",
    "\n",
    "print(\"Eval function 'eval_tool_efficiency' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Eval Functions Against Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_function(eval_fn, labeled_traces: list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Test an eval function against labeled traces and compute metrics.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - predictions: list of (trace_id, predicted_score, human_score)\n",
    "    - pearson_r: Pearson correlation coefficient\n",
    "    - accuracy: Binary accuracy at 0.5 threshold\n",
    "    - cohen_kappa: Cohen's kappa statistic\n",
    "    - f1: F1 score\n",
    "    - confusion_matrix: dict with tp, tn, fp, fn\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for trace_row in labeled_traces:\n",
    "        # Prepare trace dict\n",
    "        trace_dict = db_utils.trace_to_dict_for_eval(trace_row)\n",
    "        \n",
    "        # Prepare task dict\n",
    "        task = {\"user_message\": db_utils.extract_user_message(trace_dict) or \"\"}\n",
    "        \n",
    "        # Empty task_metadata for now\n",
    "        task_metadata = {}\n",
    "        \n",
    "        try:\n",
    "            score, feedback = eval_fn(task, task_metadata, trace_dict, ctx)\n",
    "            predictions.append({\n",
    "                \"trace_id\": trace_row[\"id\"],\n",
    "                \"predicted_score\": score,\n",
    "                \"human_score\": trace_row[\"human_score\"],\n",
    "                \"feedback\": feedback,\n",
    "                \"human_rating\": trace_row[\"rating\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating trace {trace_row['id']}: {e}\")\n",
    "            predictions.append({\n",
    "                \"trace_id\": trace_row[\"id\"],\n",
    "                \"predicted_score\": 0.5,\n",
    "                \"human_score\": trace_row[\"human_score\"],\n",
    "                \"feedback\": f\"Error: {e}\",\n",
    "                \"human_rating\": trace_row[\"rating\"]\n",
    "            })\n",
    "    \n",
    "    if not predictions:\n",
    "        return {\"error\": \"No predictions made\"}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pred_scores = np.array([p[\"predicted_score\"] for p in predictions])\n",
    "    human_scores = np.array([p[\"human_score\"] for p in predictions])\n",
    "    \n",
    "    # Pearson correlation\n",
    "    if len(set(pred_scores)) > 1 and len(set(human_scores)) > 1:\n",
    "        pearson_r, pearson_p = stats.pearsonr(pred_scores, human_scores)\n",
    "    else:\n",
    "        pearson_r, pearson_p = 0.0, 1.0\n",
    "    \n",
    "    # Binary classification at 0.5 threshold\n",
    "    pred_binary = (pred_scores >= 0.5).astype(int)\n",
    "    human_binary = (human_scores >= 0.5).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(human_binary, pred_binary)\n",
    "    kappa = cohen_kappa_score(human_binary, pred_binary)\n",
    "    f1 = f1_score(human_binary, pred_binary, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(human_binary, pred_binary, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"pearson_r\": pearson_r,\n",
    "        \"pearson_p\": pearson_p,\n",
    "        \"accuracy\": acc,\n",
    "        \"cohen_kappa\": kappa,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": {\"tp\": int(tp), \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn)}\n",
    "    }\n",
    "\n",
    "print(\"Test function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all eval functions (if we have labeled data)\n",
    "if labeled_traces:\n",
    "    eval_functions = [\n",
    "        (\"Response Length\", eval_response_length),\n",
    "        (\"No Errors\", eval_no_errors),\n",
    "        (\"Tool Efficiency\", eval_tool_efficiency),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for name, eval_fn in eval_functions:\n",
    "        print(f\"\\nTesting: {name}\")\n",
    "        result = test_eval_function(eval_fn, labeled_traces)\n",
    "        result[\"name\"] = name\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Pearson r: {result['pearson_r']:.3f} (p={result['pearson_p']:.3f})\")\n",
    "        print(f\"  Accuracy:  {result['accuracy']:.3f}\")\n",
    "        print(f\"  Cohen's k: {result['cohen_kappa']:.3f}\")\n",
    "        print(f\"  F1 Score:  {result['f1_score']:.3f}\")\n",
    "        print(f\"  Confusion: {result['confusion_matrix']}\")\n",
    "else:\n",
    "    print(\"No labeled traces available. Add feedback to traces first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Eval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "if 'results' in dir() and results:\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Eval Function\": r[\"name\"],\n",
    "            \"Pearson r\": r[\"pearson_r\"],\n",
    "            \"Accuracy\": r[\"accuracy\"],\n",
    "            \"Cohen's Kappa\": r[\"cohen_kappa\"],\n",
    "            \"F1 Score\": r[\"f1_score\"],\n",
    "            \"TP\": r[\"confusion_matrix\"][\"tp\"],\n",
    "            \"TN\": r[\"confusion_matrix\"][\"tn\"],\n",
    "            \"FP\": r[\"confusion_matrix\"][\"fp\"],\n",
    "            \"FN\": r[\"confusion_matrix\"][\"fn\"],\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    metrics = [\"Accuracy\", \"Cohen's Kappa\", \"F1 Score\"]\n",
    "    colors = ['steelblue', 'coral', 'forestgreen']\n",
    "    \n",
    "    for ax, metric, color in zip(axes, metrics, colors):\n",
    "        comparison_df.plot(kind='bar', x='Eval Function', y=metric, ax=ax, \n",
    "                          color=color, legend=False)\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to compare. Run the eval tests first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Write Your Own Eval Function\n",
    "\n",
    "Use this template to experiment with your own eval strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for custom eval function\n",
    "def my_custom_eval(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Your custom eval function.\n",
    "    \n",
    "    Args:\n",
    "        task: {\"user_message\": \"the user's request\"}\n",
    "        task_metadata: Additional context (expected_output, success_criteria, etc.)\n",
    "        trace: The agent's execution trace with steps, tool_calls, etc.\n",
    "        ctx: EvalContext with LLM access (ctx.call_llm()) and utilities\n",
    "    \n",
    "    Returns:\n",
    "        (score, feedback) where score is 0.0-1.0 and feedback explains the score\n",
    "    \"\"\"\n",
    "    # Extract useful data\n",
    "    user_message = task.get(\"user_message\", \"\")\n",
    "    assistant_response = db_utils.extract_assistant_response(trace)\n",
    "    tool_calls = db_utils.extract_tool_calls(trace)\n",
    "    \n",
    "    # Your evaluation logic here\n",
    "    # Example: Check if response mentions key terms from the user message\n",
    "    score = 0.5  # Default score\n",
    "    feedback = \"Default evaluation\"\n",
    "    \n",
    "    # TODO: Implement your custom logic\n",
    "    # For example:\n",
    "    # - Check response relevance to user message\n",
    "    # - Verify tool usage is appropriate\n",
    "    # - Check for specific patterns or content\n",
    "    # - Use ctx.call_llm() for semantic evaluation\n",
    "    \n",
    "    return (score, feedback)\n",
    "\n",
    "# Test your custom eval\n",
    "if labeled_traces:\n",
    "    print(\"Testing custom eval...\")\n",
    "    result = test_eval_function(my_custom_eval, labeled_traces[:10])  # Test on first 10\n",
    "    print(f\"Accuracy: {result['accuracy']:.3f}\")\n",
    "    print(f\"Cohen's Kappa: {result['cohen_kappa']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LLM-as-Judge Eval (Advanced)\n",
    "\n",
    "If you have API keys set up, you can use LLMs for semantic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-Judge eval function (requires API keys)\n",
    "def eval_llm_judge(task: dict, task_metadata: dict, trace: dict, ctx) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Use an LLM to evaluate the trace quality.\n",
    "    \"\"\"\n",
    "    user_message = task.get(\"user_message\", \"\")\n",
    "    assistant_response = db_utils.extract_assistant_response(trace)\n",
    "    \n",
    "    if not assistant_response:\n",
    "        return (0.0, \"No response to evaluate\")\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate this AI assistant response on a scale of 0-10.\n",
    "\n",
    "User Request: {user_message[:500]}\n",
    "\n",
    "Assistant Response: {assistant_response[:1000]}\n",
    "\n",
    "Rate the response based on:\n",
    "1. Relevance: Does it address the user's request?\n",
    "2. Completeness: Is it thorough?\n",
    "3. Accuracy: Is the information correct?\n",
    "4. Clarity: Is it well-written?\n",
    "\n",
    "Respond with ONLY a JSON object:\n",
    "{{\"score\": <0-10>, \"reasoning\": \"<brief explanation>\"}}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ctx.call_llm(prompt, model=\"gpt-4o-mini\", temperature=0)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(response)\n",
    "        score = result.get(\"score\", 5) / 10.0  # Normalize to 0-1\n",
    "        reasoning = result.get(\"reasoning\", \"No reasoning provided\")\n",
    "        \n",
    "        return (score, reasoning)\n",
    "    except Exception as e:\n",
    "        return (0.5, f\"LLM evaluation failed: {e}\")\n",
    "\n",
    "# Test LLM judge (only if API is available)\n",
    "if OPENAI_AVAILABLE and labeled_traces:\n",
    "    # Initialize OpenAI client (add your API key)\n",
    "    # ctx.openai = OpenAI(api_key=\"your-api-key\")\n",
    "    \n",
    "    print(\"LLM-as-Judge eval defined. Set ctx.openai to test.\")\n",
    "else:\n",
    "    print(\"OpenAI not available or no labeled traces. Skipping LLM eval test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Save your best eval function for use in the main system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export an eval function to a Python file\n",
    "def export_eval_function(eval_fn, filepath: str):\n",
    "    \"\"\"Export an eval function to a standalone Python file.\"\"\"\n",
    "    import inspect\n",
    "    source = inspect.getsource(eval_fn)\n",
    "    \n",
    "    template = '''\"\"\"Auto-generated eval function from notebook experimentation.\"\"\"\n",
    "import json\n",
    "from typing import Any, Optional\n",
    "\n",
    "{source}\n",
    "\n",
    "# Main function that will be called by the eval runner\n",
    "eval_function = {fn_name}\n",
    "'''.format(source=source, fn_name=eval_fn.__name__)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(template)\n",
    "    \n",
    "    print(f\"Exported to {filepath}\")\n",
    "\n",
    "# Example: export_eval_function(eval_response_length, \"my_eval.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add more labeled traces**: Label traces in the iofold UI to get more training data\n",
    "2. **Experiment with different strategies**: Try combining multiple signals\n",
    "3. **Test LLM-as-judge**: Set up API keys and try semantic evaluation\n",
    "4. **Export successful evals**: Use the export function to save good eval functions\n",
    "5. **Integrate with main system**: Use the exported eval in the iofold UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
